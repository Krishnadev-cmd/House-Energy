{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4cb929",
   "metadata": {},
   "source": [
    "# AWS SageMaker Training & Deployment Pipeline\n",
    "\n",
    "## How It Works:\n",
    "\n",
    "### 1. **Training Script (train.py)**\n",
    "This notebook contains your training code that will run on SageMaker. The script:\n",
    "- Loads data from S3 (SageMaker downloads it automatically)\n",
    "- Trains a Random Forest model\n",
    "- Saves the model artifacts\n",
    "- Uses special SageMaker environment variables:\n",
    "  - `SM_CHANNEL_TRAINING`: Directory where training data is downloaded\n",
    "  - `SM_MODEL_DIR`: Directory to save the trained model\n",
    "  - `SM_OUTPUT_DATA_DIR`: Directory for training metrics/outputs\n",
    "\n",
    "### 2. **The Workflow:**\n",
    "\n",
    "```\n",
    "Your S3 Bucket (Parquet files)\n",
    "    ‚Üì\n",
    "SageMaker Training Job\n",
    "    ‚Üì (Downloads data from S3)\n",
    "Training Instance (runs train.py)\n",
    "    ‚Üì (Trains model)\n",
    "Model Artifacts saved to S3\n",
    "    ‚Üì\n",
    "Create SageMaker Model\n",
    "    ‚Üì\n",
    "Deploy to SageMaker Endpoint\n",
    "    ‚Üì\n",
    "Real-time Predictions via API\n",
    "```\n",
    "\n",
    "### 3. **What You Need:**\n",
    "- ‚úÖ Training script (this file converted to .py)\n",
    "- ‚úÖ Parquet data uploaded to S3\n",
    "- ‚úÖ IAM Role with SageMaker permissions\n",
    "- ‚úÖ Inference script (for deployment)\n",
    "- ‚úÖ SageMaker SDK to orchestrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a6a4e",
   "metadata": {},
   "source": [
    "### Step 1: Create a SageMaker Orchestration Script\n",
    "This script will launch the training job and deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5925bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Ambika M\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Ambika M\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name Krishnadev to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using role: arn:aws:iam::205999347239:role/SageMakerExecutionRole-EnergyForecast\n",
      "Using bucket: energy-forecast-processed-krishnadev\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "bucket = 'energy-forecast-processed-krishnadev'  # REPLACE with your bucket name\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Get execution role (or specify ARN directly)\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except:\n",
    "    # If running outside SageMaker, use the role ARN you created\n",
    "    role = 'arn:aws:iam::205999347239:role/SageMakerExecutionRole-EnergyForecast'\n",
    "    # Get your account ID from: aws sts get-caller-identity\n",
    "\n",
    "print(f\"Using role: {role}\")\n",
    "print(f\"Using bucket: {bucket}\")\n",
    "\n",
    "# Define training data location\n",
    "train_data = 's3://energy-forecast-processed-krishnadev/features/energy_features.parquet/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07662752",
   "metadata": {},
   "source": [
    "### Step 4: Create and Run the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89f6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2025-10-23-16-56-05-998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-23 16:56:16 Starting - Starting the training job......\n",
      "2025-10-23 16:56:48 Downloading - Downloading input data\n",
      "2025-10-23 16:56:48 Downloading - Downloading input data......\n",
      "2025-10-23 16:57:08 Downloading - Downloading the training image\n",
      "2025-10-23 16:57:08 Downloading - Downloading the training image......\n",
      "2025-10-23 16:57:54 Training - Training image download completed. Training in progress.\n",
      "2025-10-23 16:57:54 Training - Training image download completed. Training in progress.......\n",
      "2025-10-23 16:58:27 Uploading - Uploading generated training model\n",
      "2025-10-23 16:58:27 Completed - Training job completed\n",
      "\n",
      "2025-10-23 16:58:27 Uploading - Uploading generated training model\n",
      "2025-10-23 16:58:27 Completed - Training job completed\n",
      "....Training seconds: 99\n",
      "Billable seconds: 99\n",
      "Training seconds: 99\n",
      "Billable seconds: 99\n",
      "Training completed!\n",
      "Model artifacts saved to: s3://energy-forecast-models-krishnadev/sagemaker-scikit-learn-2025-10-23-16-56-05-998/output/model.tar.gz\n",
      "Training completed!\n",
      "Model artifacts saved to: s3://energy-forecast-models-krishnadev/sagemaker-scikit-learn-2025-10-23-16-56-05-998/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Create SKLearn estimator for training\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='training.py',              # Your training script\n",
    "    source_dir='sagemaker_scripts',         # Only include training scripts (NOT entire directory)\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',        # Training instance type\n",
    "    instance_count=1,\n",
    "    framework_version='1.2-1',           # Scikit-learn version\n",
    "    py_version='py3',\n",
    "    output_path='s3://energy-forecast-models-krishnadev',\n",
    "    sagemaker_session=session,\n",
    "    hyperparameters={\n",
    "        # You can pass hyperparameters here\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start training job\n",
    "print(\"Starting training job...\")\n",
    "sklearn_estimator.fit({'training': train_data})\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Model artifacts saved to: {sklearn_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5944a3e",
   "metadata": {},
   "source": [
    "### Step 5: Deploy the Model to an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d51f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to endpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2025-10-23-16-58-37-432\n",
      "INFO:sagemaker:Creating endpoint-config with name energy-model-20251023-222837\n",
      "INFO:sagemaker:Creating endpoint-config with name energy-model-20251023-222837\n",
      "INFO:sagemaker:Creating endpoint with name energy-model-20251023-222837\n",
      "INFO:sagemaker:Creating endpoint with name energy-model-20251023-222837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!Model deployed to endpoint: energy-model-20251023-222837\n",
      "Endpoint is ready for predictions!\n",
      "!Model deployed to endpoint: energy-model-20251023-222837\n",
      "Endpoint is ready for predictions!\n"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "print(\"Deploying model to endpoint...\")\n",
    "\n",
    "predictor = sklearn_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',         # Inference instance type\n",
    "    endpoint_name=f'energy-model-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "    entry_point='inference.py'  # Your inference script\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")\n",
    "print(\"Endpoint is ready for predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4d1f4",
   "metadata": {},
   "source": [
    "### Step 6: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854d1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error making prediction: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<title>500 Internal Server Error</title>\n",
      "<h1>Internal Server Error</h1>\n",
      "<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n",
      "\". See https://ap-south-1.console.aws.amazon.com/cloudwatch/home?region=ap-south-1#logEventViewer:group=/aws/sagemaker/Endpoints/energy-model-20251023-222837 in account 205999347239 for more information.\n",
      "Check CloudWatch logs for details\n"
     ]
    }
   ],
   "source": [
    "# Configure serializers for proper JSON handling\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "# Example prediction - send as list of dictionaries\n",
    "test_data = [\n",
    "    {\n",
    "        'hour': 10,\n",
    "        'day_of_week': 1,\n",
    "        'day': 15,\n",
    "        'month': 6,\n",
    "        'is_weekend': 0,\n",
    "        'Global_active_power_max': 4.5,\n",
    "        'Global_active_power_min': 0.5,\n",
    "        'Global_active_power_std': 1.2,\n",
    "        'Voltage_mean': 240.5,\n",
    "        'power_lag_1h': 3.2,\n",
    "        'power_lag_24h': 3.5,\n",
    "        'power_lag_168h': 3.1,\n",
    "        'power_rolling_mean_7d': 3.3,\n",
    "        'power_rolling_std_7d': 0.8\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Sending prediction request...\")\n",
    "print(f\"Input data: {test_data}\\n\")\n",
    "\n",
    "# Make prediction\n",
    "try:\n",
    "    result = predictor.predict(test_data)\n",
    "    print(f\"‚úÖ Prediction successful!\")\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error making prediction: {str(e)}\")\n",
    "    print(\"\\nüîç Check CloudWatch logs below for detailed error messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128162e6",
   "metadata": {},
   "source": [
    "### Step 6.5: Check CloudWatch Logs for Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da490e",
   "metadata": {},
   "source": [
    "### Step 6.25: Alternative - Test with JSON String Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with proper serialization\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Set serializers\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "# Test data - make sure it matches the exact feature order from training\n",
    "test_data = [\n",
    "    {\n",
    "        'hour': 10,\n",
    "        'day_of_week': 1,\n",
    "        'day': 15,\n",
    "        'month': 6,\n",
    "        'is_weekend': 0,\n",
    "        'Global_active_power_max': 4.5,\n",
    "        'Global_active_power_min': 0.5,\n",
    "        'Global_active_power_std': 1.2,\n",
    "        'Voltage_mean': 240.5,\n",
    "        'power_lag_1h': 3.2,\n",
    "        'power_lag_24h': 3.5,\n",
    "        'power_lag_168h': 3.1,\n",
    "        'power_rolling_mean_7d': 3.3,\n",
    "        'power_rolling_std_7d': 0.8\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Sending prediction request...\")\n",
    "print(f\"Input data: {test_data}\\n\")\n",
    "\n",
    "try:\n",
    "    result = predictor.predict(test_data)\n",
    "    print(f\"‚úÖ Prediction successful!\")\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error making prediction: {str(e)}\")\n",
    "    print(\"\\nüîç This error indicates the inference script has an issue.\")\n",
    "    print(\"Check the CloudWatch logs cell below for detailed error messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea72d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: InService\n",
      "Endpoint ARN: arn:aws:sagemaker:ap-south-1:205999347239:endpoint/energy-model-20251023-222837\n",
      "\n",
      "Looking for log group: /aws/sagemaker/Endpoints/energy-model-20251023-222837\n",
      "\n",
      "Available SageMaker Endpoint log groups:\n",
      "\n",
      "‚ö†Ô∏è  Log group doesn't exist yet.\n",
      "\n",
      "This means:\n",
      "  - The endpoint hasn't generated any logs yet\n",
      "  - You need to wait a few minutes after endpoint creation\n",
      "  - Or the endpoint may not be fully deployed\n",
      "\n",
      "üí° Steps to troubleshoot:\n",
      "  1. Wait 2-3 minutes after endpoint creation\n",
      "  2. Make a prediction request to generate logs\n",
      "  3. Run this cell again\n",
      "\n",
      "üìä Manual check: https://ap-south-1.console.aws.amazon.com/cloudwatch/home?region=ap-south-1#logsV2:log-groups\n",
      "Available SageMaker Endpoint log groups:\n",
      "\n",
      "‚ö†Ô∏è  Log group doesn't exist yet.\n",
      "\n",
      "This means:\n",
      "  - The endpoint hasn't generated any logs yet\n",
      "  - You need to wait a few minutes after endpoint creation\n",
      "  - Or the endpoint may not be fully deployed\n",
      "\n",
      "üí° Steps to troubleshoot:\n",
      "  1. Wait 2-3 minutes after endpoint creation\n",
      "  2. Make a prediction request to generate logs\n",
      "  3. Run this cell again\n",
      "\n",
      "üìä Manual check: https://ap-south-1.console.aws.amazon.com/cloudwatch/home?region=ap-south-1#logsV2:log-groups\n"
     ]
    }
   ],
   "source": [
    "# Fetch recent CloudWatch logs for the endpoint\n",
    "import time\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=region)\n",
    "\n",
    "# Check if endpoint exists and get its status\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "try:\n",
    "    endpoint_desc = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "    print(f\"Endpoint Status: {endpoint_desc['EndpointStatus']}\")\n",
    "    print(f\"Endpoint ARN: {endpoint_desc['EndpointArn']}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error describing endpoint: {str(e)}\\n\")\n",
    "\n",
    "# Try to find the log group\n",
    "log_group = f'/aws/sagemaker/Endpoints/{predictor.endpoint_name}'\n",
    "print(f\"Looking for log group: {log_group}\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if log group exists\n",
    "    log_groups = logs_client.describe_log_groups(\n",
    "        logGroupNamePrefix='/aws/sagemaker/Endpoints/'\n",
    "    )\n",
    "    \n",
    "    print(\"Available SageMaker Endpoint log groups:\")\n",
    "    for lg in log_groups['logGroups']:\n",
    "        print(f\"  - {lg['logGroupName']}\")\n",
    "    print()\n",
    "    \n",
    "    # Try to get log streams\n",
    "    streams = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True,\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(streams['logStreams'])} log streams\\n\")\n",
    "    \n",
    "    # Get recent logs from the most recent stream\n",
    "    if streams['logStreams']:\n",
    "        stream_name = streams['logStreams'][0]['logStreamName']\n",
    "        print(f\"Reading from stream: {stream_name}\\n\")\n",
    "        \n",
    "        events = logs_client.get_log_events(\n",
    "            logGroupName=log_group,\n",
    "            logStreamName=stream_name,\n",
    "            limit=50,\n",
    "            startFromHead=False\n",
    "        )\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"RECENT LOGS:\")\n",
    "        print(\"=\" * 80)\n",
    "        for event in events['events'][-20:]:  # Last 20 log messages\n",
    "            print(event['message'])\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No log streams found yet.\")\n",
    "        print(\"This is normal if:\")\n",
    "        print(\"  1. The endpoint was just created\")\n",
    "        print(\"  2. No prediction requests have been made yet\")\n",
    "        print(\"  3. Logs are still being initialized\")\n",
    "        print(\"\\nüí° Try making a prediction request first, then check logs again.\")\n",
    "        \n",
    "except logs_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"‚ö†Ô∏è  Log group doesn't exist yet.\")\n",
    "    print(\"\\nThis means:\")\n",
    "    print(\"  - The endpoint hasn't generated any logs yet\")\n",
    "    print(\"  - You need to wait a few minutes after endpoint creation\")\n",
    "    print(\"  - Or the endpoint may not be fully deployed\")\n",
    "    print(\"\\nüí° Steps to troubleshoot:\")\n",
    "    print(\"  1. Wait 2-3 minutes after endpoint creation\")\n",
    "    print(\"  2. Make a prediction request to generate logs\")\n",
    "    print(\"  3. Run this cell again\")\n",
    "    print(f\"\\nüìä Manual check: https://ap-south-1.console.aws.amazon.com/cloudwatch/home?region=ap-south-1#logsV2:log-groups\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching logs: {str(e)}\")\n",
    "    print(f\"\\nüìä Manually check logs at:\")\n",
    "    print(f\"https://ap-south-1.console.aws.amazon.com/cloudwatch/home?region=ap-south-1#logsV2:log-groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b798540",
   "metadata": {},
   "source": [
    "### Step 7: Clean Up (When Done Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c59481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint to avoid charges\n",
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ea98b",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: How It All Works\n",
    "\n",
    "1. **You write** `train.py` (the training logic - cells 1-4 above)\n",
    "2. **SageMaker receives** your script and S3 data location\n",
    "3. **SageMaker launches** a training instance (EC2 machine)\n",
    "4. **SageMaker downloads** your parquet files from S3 to the instance\n",
    "5. **Your script runs** on that instance with the data\n",
    "6. **Model artifacts** are saved and uploaded back to S3\n",
    "7. **You deploy** by creating a SageMaker Model + Endpoint\n",
    "8. **SageMaker creates** an inference instance running your model\n",
    "9. **You send requests** to the endpoint URL to get predictions\n",
    "10. **The endpoint** loads your model and returns predictions\n",
    "\n",
    "### Key Benefits:\n",
    "- ‚úÖ No need to manage servers\n",
    "- ‚úÖ Automatic scaling\n",
    "- ‚úÖ Built-in logging and monitoring\n",
    "- ‚úÖ Version control for models\n",
    "- ‚úÖ A/B testing capabilities\n",
    "- ‚úÖ Pay only for what you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb40d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb2eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5f3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
